import jax.numpy as jnp
import flax.linen as nn
import jax
import time
from flax.core import FrozenDict
import flax
from enum import IntEnum
from typing import Optional, Tuple, Any
from einops import rearrange

class Modality(IntEnum):
    LATENT   = -1
    IMAGE    = 0
    ACTION   = 1
    PROPRIO  = 2
    REGISTER = 3
    SPATIAL = 4
    SHORTCUT_SIGNAL = 5
    SHORTCUT_STEP = 6

    # add more as needed

@flax.struct.dataclass  # immutable, PyTree-friendly
class TokenLayout:
    """
    Ordered token layout for a single timestep: latents first (if any),
    then a sequence of (modality, count) segments.
    """
    n_latents: int
    segments: Tuple[Tuple[Modality, int], ...]  # e.g., ((Modality.IMAGE, n_patches), (Modality.ACTION, n_act), ...)

    def S(self) -> int:
        return self.n_latents + sum(n for _, n in self.segments)

    def modality_ids(self) -> jnp.ndarray:
        parts = [jnp.full((self.n_latents,), Modality.LATENT, dtype=jnp.int32)] if self.n_latents > 0 else []
        for m, n in self.segments:
            if n > 0:
                parts.append(jnp.full((n,), int(m), dtype=jnp.int32))
        return jnp.concatenate(parts) if parts else jnp.zeros((0,), dtype=jnp.int32)  # (S,)

    def slices(self) -> dict:
        """Convenience: start/stop indices per modality (first occurrence if repeated)."""
        idx = 0
        out = {}
        if self.n_latents > 0:
            out[Modality.LATENT] = slice(idx, idx + self.n_latents); idx += self.n_latents
        for m, n in self.segments:
            if n > 0 and m not in out:
                out[m] = slice(idx, idx + n)
            idx += n
        return out

    
def sinusoid_table(n: int, d: int, base: float = 10000.0, dtype=jnp.float32) -> jnp.ndarray:
    """
    Standard Transformer sinusoid: even dims use sin, odd dims use cos with frequencies
    base^{-2k/d}. Works for odd d too.
    """
    pos = jnp.arange(n, dtype=dtype)[:, None]            # (n,1)
    i = jnp.arange(d, dtype=dtype)[None, :]              # (1,d)
    # k = floor(i/2)
    k = jnp.floor(i / 2.0)
    div = jnp.power(base, -(2.0 * k) / jnp.maximum(1.0, jnp.array(d, dtype)))
    angles = pos * div                                    # (n,d)
    table = jnp.where((i % 2) == 0, jnp.sin(angles), jnp.cos(angles))
    return table.astype(dtype)


def add_sinusoidal_positions(tokens_btSd: jnp.ndarray) -> jnp.ndarray:
    """tokens: (B,T,S,D) -> adds time and step sinusoids and returns same shape."""
    B, T, S, D = tokens_btSd.shape
    pos_t = sinusoid_table(T, D)     # (T,D)
    pos_s = sinusoid_table(S, D)     # (S,D)
    # Optionally scale to keep variance stable (common trick)
    scale = 1.0 / jnp.sqrt(jnp.array(D, dtype=tokens_btSd.dtype))
    return tokens_btSd + scale * (pos_t[None, :, None, :] + pos_s[None, None, :, :])

class MAEReplacer(nn.Module):
    p_min: float = 0.0
    p_max: float = 0.9

    @nn.compact
    def __call__(self, patches_btnd: jnp.ndarray) -> tuple[jnp.ndarray, jnp.ndarray]:
        # patches_btnd: (B,T,Np,D)
        B, T, Np, D = patches_btnd.shape
        mask_token = self.param("mask_token", nn.initializers.normal(0.02), (D,))
        # draw RNGs from a named stream
        rng = self.make_rng("mae")
        p_rng, m_rng = jax.random.split(rng)
        p_bt = jax.random.uniform(p_rng, (B, T), minval=self.p_min, maxval=self.p_max)  # (B,T)
        keep_prob_bt1 = 1.0 - p_bt[..., None]                                           # (B,T,1)
        keep = jax.random.bernoulli(m_rng, keep_prob_bt1, (B, T, Np))                   # (B,T,Np)
        keep = keep[..., None]                                                          # (B,T,Np,1)
        replaced = jnp.where(keep, patches_btnd, mask_token.reshape(1, 1, 1, D))
        mae_mask = (~keep).astype(jnp.bool_)                                            # (B,T,Np,1)
        return replaced, mae_mask, keep_prob_bt1


# ---------- small building blocks ----------

class RMSNorm(nn.Module):
    eps: float = 1e-6
    @nn.compact
    def __call__(self, x):
        scale = self.param("scale", nn.initializers.ones, (x.shape[-1],))
        var = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
        return x * (scale / jnp.sqrt(var + self.eps))

class MLP(nn.Module):
    """
    Transformer MLP with optional SwiGLU gating.

    Args:
      d_model:     input/output width of the block (last-dim of x).
      mlp_ratio:   expansion factor for hidden size if NOT using 2/3 parity.
      dropout:     dropout rate applied after activation and after output proj.
      swiglu:      if True, use SwiGLU; else standard GELU MLP.
      parity_2over3:
                   if True and swiglu=True, set hidden = (2/3)*mlp_ratio*d_model
                   to roughly match parameter count of a GELU MLP with mlp_ratio.
      dtype:       param/compute dtype.
    """
    d_model: int
    mlp_ratio: float = 4.0
    dropout: float = 0.0
    swiglu: bool = True
    parity_2over3: bool = False
    dtype: Any = jnp.float32

    @nn.compact
    def __call__(self, x: jnp.ndarray, *, deterministic: bool) -> jnp.ndarray:
        """
        Args:
          x:            (..., d_model) input activations.
          deterministic:
                        True disables dropout (eval); False enables dropout (train).

        Returns:
          y:            (..., d_model) output activations, same shape as input.
        """
        # Choose hidden size
        mult = self.mlp_ratio
        if self.swiglu and self.parity_2over3:
            mult = self.mlp_ratio * (2.0 / 3.0)  # param parity with GELU MLP

        hidden = int(self.d_model * mult)

        if self.swiglu:
            # SwiGLU: Dense -> split -> u * silu(v)
            pre = nn.Dense(
                2 * hidden, dtype=self.dtype, name="fc_in"
            )(x)  # (..., 2H)
            u, v = jnp.split(pre, 2, axis=-1)     # (..., H), (..., H)
            h = u * jax.nn.silu(v)                # (..., H)
        else:
            # Standard GELU MLP
            h = nn.Dense(hidden, dtype=self.dtype, name="fc_in")(x)
            h = nn.gelu(h)

        h = nn.Dropout(self.dropout)(h, deterministic=deterministic)
        y = nn.Dense(self.d_model, dtype=self.dtype, name="fc_out")(h)
        y = nn.Dropout(self.dropout)(y, deterministic=deterministic)
        return y
# ---------- axial attention layers ----------
class SpaceSelfAttentionModality(nn.Module):
    """
    Space self-attention with modality routing.

    Args:
      d_model: int
      n_heads: int
      modality_ids: jnp.ndarray with shape (S,), per-token modality id for the S tokens.
                    Convention: latents are the first `n_latents` tokens; you can set their id to -1.
      n_latents: int, number of latent tokens at the beginning of S.
      mode: str in {"encoder", "decoder"}.
            - "encoder": latents→all; non-latents→same-modality only.
            - "decoder": latents→latents-only; non-latents→same-modality + latents.
      dropout: float
    """
    d_model: int
    n_heads: int
    modality_ids: jnp.ndarray  # (S,)
    n_latents: int
    mode: str = "encoder"      # or "decoder"
    dropout: float = 0.0

    def setup(self):
        # Cache a (S,S) boolean mask indicating allowed key for each query index, per mode.
        S = int(self.modality_ids.shape[0])

        # Broadcast helpers
        q_idx = jnp.arange(S)[:, None]       # (S,1)
        k_idx = jnp.arange(S)[None, :]       # (1,S)

        is_q_lat = q_idx < self.n_latents     # (S,1) bool
        is_k_lat = k_idx < self.n_latents     # (1,S) bool

        q_mod = self.modality_ids[q_idx]      # (S,1)
        k_mod = self.modality_ids[k_idx]      # (1,S)
        same_mod = (q_mod == k_mod)           # (S,S)

        if self.mode == "encoder":
            # latents -> all; non-latents -> same modality only (no access to latents unless same modality==latent, which they aren't)
            allow_lat_q = jnp.ones((S, S), dtype=bool)             # lat q attends to everything
            allow_nonlat_q = same_mod                              # non-lat q attends within itself only
            mask = jnp.where(is_q_lat, allow_lat_q, allow_nonlat_q)
        elif self.mode == "decoder":
            # latents -> latents only; non-latents -> same modality OR latents
            allow_lat_q = is_k_lat                                  # lat q -> lat k only
            allow_nonlat_q = jnp.logical_or(same_mod, is_k_lat)     # non-lat q -> same mod + latents
            mask = jnp.where(is_q_lat, allow_lat_q, allow_nonlat_q)
        elif self.mode == "dynamics":
            # allow full attention
            mask = jnp.ones((S, S), dtype=bool)
        else:
            raise ValueError(f"Unknown mode {self.mode}")

        # Save (1,1,S,S) so it broadcasts over batch*time and heads -> (B*T, 1, S, S)
        modality_mask = mask[None, None, :, :]                   # (1,1,S,S)
        self.modality_mask = self.variable("constants", "modality_mask", lambda: modality_mask)

    @nn.compact
    def __call__(self, x, *, deterministic: bool):
        # x: (B, T, S, D)  -> attention across S within each (B,T)
        B, T, S, D = x.shape
        x_ = x.reshape(B*T, S, D)

        # Flax MHA mask shape can be (batch, num_heads, q_len, k_len). We want one mask per (B*T).
        mask = jnp.broadcast_to(self.modality_mask.value, (B*T, 1, S, S))   # (B*T,1,S,S)

        y_ = nn.MultiHeadDotProductAttention(
            num_heads=self.n_heads,
            qkv_features=self.d_model,
            dropout_rate=self.dropout,
            deterministic=deterministic,
        )(x_, x_, mask=mask)

        y = y_.reshape(B, T, S, D)
        return y

class TimeSelfAttention(nn.Module):
    d_model: int
    n_heads: int
    dropout: float = 0.0
    latents_only: bool = True
    n_latents: int = 0   # required if latents_only

    @nn.compact
    def __call__(self, x, *, deterministic: bool):
        # x: (B, T, S, D) -> attend across T, causal
        B, T, S, D = x.shape
        if self.latents_only:
            assert 0 < self.n_latents <= S
            lat = x[:, :, :self.n_latents, :]                # (B, T, L, D)
            lat_btld = lat.transpose(0, 2, 1, 3).reshape(B*self.n_latents, T, D)  # (B*L, T, D)
            causal = nn.attention.make_causal_mask(jnp.ones((B*self.n_latents, T), dtype=bool))
            out = nn.MultiHeadDotProductAttention(
                num_heads=self.n_heads,
                qkv_features=self.d_model,
                dropout_rate=self.dropout,
                deterministic=deterministic,
            )(lat_btld, lat_btld, mask=causal)
            out = out.reshape(B, self.n_latents, T, D).transpose(0, 2, 1, 3)      # back to (B, T, L, D)
            x = x.at[:, :, :self.n_latents, :].set(out)
            return x
        else:
            x_bstd = x.transpose(0, 2, 1, 3).reshape(B*S, T, D)  # (B*S, T, D)
            causal = nn.attention.make_causal_mask(jnp.ones((B*S, T), dtype=bool))
            out = nn.MultiHeadDotProductAttention(
                num_heads=self.n_heads,
                qkv_features=self.d_model,
                dropout_rate=self.dropout,
                deterministic=deterministic,
            )(x_bstd, x_bstd, mask=causal)
            out = out.reshape(B, S, T, D).transpose(0, 2, 1, 3)  # back to (B, T, S, D)
            return out

# ---------- a single block-causal layer ----------
class BlockCausalLayer(nn.Module):
    d_model: int
    n_heads: int
    n_latents: int
    modality_ids: jnp.ndarray     # (S,)
    space_mode: str               # "encoder" or "decoder"
    dropout: float = 0.0
    mlp_ratio: float = 4.0
    layer_index: int = 0
    time_every: int = 4
    latents_only_time: bool = True

    @nn.compact
    def __call__(self, x, *, deterministic: bool):
        # --- Space attention (within timestep, modality-aware) ---
        y = RMSNorm()(x)
        y = SpaceSelfAttentionModality(
            d_model=self.d_model,
            n_heads=self.n_heads,
            modality_ids=self.modality_ids,
            n_latents=self.n_latents,
            mode=self.space_mode,
            dropout=self.dropout,
        )(y, deterministic=deterministic)
        x = x + nn.Dropout(self.dropout)(y, deterministic=deterministic)

        # --- Time attention (causal across timesteps), only on some layers ---
        if (self.layer_index + 1) % self.time_every == 0:
            y = RMSNorm()(x)
            y = TimeSelfAttention(
                self.d_model, self.n_heads, self.dropout,
                latents_only=self.latents_only_time, n_latents=self.n_latents
            )(y, deterministic=deterministic)
            x = x + nn.Dropout(self.dropout)(y, deterministic=deterministic)

        # --- MLP ---
        y = RMSNorm()(x)
        y = MLP(self.d_model, self.mlp_ratio, self.dropout)(y, deterministic=deterministic)
        x = x + nn.Dropout(self.dropout)(y, deterministic=deterministic)
        return x
# ---------- the transformer stack ----------

class BlockCausalTransformer(nn.Module):
    d_model: int
    n_heads: int
    depth: int
    n_latents: int
    modality_ids: jnp.ndarray   # (S,)
    space_mode: str             # "encoder" or "decoder"
    dropout: float = 0.0
    mlp_ratio: float = 4.0
    time_every: int = 4
    latents_only_time: bool = True

    @nn.compact
    def __call__(self, x, *, deterministic: bool):
        for i in range(self.depth):
            x = BlockCausalLayer(
                self.d_model, self.n_heads, self.n_latents,
                modality_ids=self.modality_ids,
                space_mode=self.space_mode,
                dropout=self.dropout, mlp_ratio=self.mlp_ratio,
                layer_index=i, time_every=self.time_every,
                latents_only_time=self.latents_only_time,
            )(x, deterministic=deterministic)
        return x

class Encoder(nn.Module):
    d_model: int
    n_latents: int
    n_patches: int
    n_heads: int
    depth: int
    d_bottleneck: int
    dropout: float = 0.0
    mlp_ratio: float = 4.0
    time_every: int = 4
    latents_only_time: bool = True
    mae_p_min: float = 0.0
    mae_p_max: float = 0.9
    
    def setup(self):
        self.patch_proj = nn.Dense(self.d_model, name="patch_proj")
        self.bottleneck_proj = nn.Dense(self.d_bottleneck, name="bottleneck_proj")
        self.layout = TokenLayout(n_latents=self.n_latents, segments=((Modality.IMAGE, self.n_patches),))
        self.modality_ids = self.layout.modality_ids()            # (S,)
        self.transformer = BlockCausalTransformer(
            d_model=self.d_model,
            n_heads=self.n_heads,
            depth=self.depth,
            n_latents=self.n_latents,
            modality_ids=self.modality_ids,
            space_mode="encoder",                 # << encoder routing
            dropout=self.dropout, mlp_ratio=self.mlp_ratio,
            time_every=self.time_every,
            latents_only_time=self.latents_only_time,
        )
        self.latents = self.param("latents_enc", nn.initializers.normal(0.02), (self.n_latents, self.d_model))

    @nn.compact
    def __call__(self, patch_tokens, *, deterministic: bool = True) -> tuple[jnp.ndarray, tuple[jnp.ndarray, jnp.ndarray]]:
        # 1) Project patches to D_model
        proj_patches = self.patch_proj(patch_tokens)  # (B,T,Np,D)

        # 2) MAE mask-and-replace on patch tokens (encoder input only)
        proj_patches_masked, patch_mask, keep_prob = MAEReplacer(name="mae", p_min=self.mae_p_min, p_max=self.mae_p_max)(proj_patches)
        # print(f"proj_patches_masked.shape: {proj_patches_masked.shape}")
        # print(f"patch_mask.shape: {patch_mask.shape}")

        # 3) Prepend learned latents (owned here)
        # print(f"latents.shape: {latents.shape}")
        B, T = proj_patches_masked.shape[:2]
        latents = jnp.broadcast_to(self.latents[None, None, ...], (B, T, *self.latents.shape))
        # print(f"lat_btld.shape: {lat_btld.shape}")
        tokens = jnp.concatenate([latents, proj_patches_masked], axis=2)  # (B,T,S=(Np+Nl),D)
        # print(f"tokens_btSd.shape: {tokens_btSd.shape}")

        # 4) Add sinusoidal positions (param-free)
        tokens = add_sinusoidal_positions(tokens)

        # 5) Feed tokens into transformer
        encoded_tokens = self.transformer(tokens, deterministic=deterministic)
        # print(f"encoded_tokens_btSd.shape: {encoded_tokens_btSd.shape}")

        # 6) Project latent tokens to bottleneck and tanh
        latent_tokens = encoded_tokens[:, :, :self.n_latents, :]
        proj_tokens = nn.tanh(self.bottleneck_proj(latent_tokens))

        return proj_tokens, (patch_mask, keep_prob)  # keep mask if you want diagnostics

class Decoder(nn.Module):
    """
    MAE-style decoder that reads temporal info from latent tokens and writes
    reconstructions at per-patch query tokens.

    Inputs:
      - z: (B, T, N_l, d_bottleneck)  -- encoder bottleneck output

    Config:
      - n_patches: number of patch query tokens to use in the decoder
      - d_patch:   dimensionality of each patch to reconstruct (D_patch)
      - d_model, n_heads, depth, dropout, mlp_ratio, time_every, latents_only_time
        typically mirror the encoder.
    """
    d_model: int
    n_heads: int
    depth: int
    n_latents: int
    n_patches: int
    d_patch: int
    dropout: float = 0.0
    mlp_ratio: float = 4.0
    time_every: int = 4
    latents_only_time: bool = True

    def setup(self):
        self.layout = TokenLayout(n_latents=self.n_latents, segments=((Modality.IMAGE, self.n_patches),))
        self.modality_ids = self.layout.modality_ids()
        self.up_proj = nn.Dense(self.d_model, name="up_proj")
        self.patch_queries = self.param(
            "patch_queries",
            nn.initializers.normal(0.02),
            (self.n_patches, self.d_model),
        ) # (Np, D)
        self.patch_head = nn.Dense(self.d_patch, name="patch_head") # (Np, D_patch)
        self.transformer = BlockCausalTransformer(
            d_model=self.d_model,
            n_heads=self.n_heads,
            depth=self.depth,
            n_latents=self.n_latents,
            modality_ids=self.modality_ids,
            space_mode="decoder",                 # << decoder routing
            dropout=self.dropout,
            mlp_ratio=self.mlp_ratio,
            time_every=self.time_every,
            latents_only_time=self.latents_only_time,
        )

    @nn.compact
    def __call__(self, z: jnp.ndarray, *, deterministic: bool = True) -> jnp.ndarray:
        B, T, N_l, d_bottleneck = z.shape

        # 1) Up-project latent bottleneck to d_model (per latent token)
        latents = nn.tanh(self.up_proj(z))  # (B, T, N_l, D)

        # 2) Learned per-patch query tokens (owned by the decoder)
        patches = jnp.broadcast_to(
            self.patch_queries[None, None, ...],
            (B, T, self.n_patches, self.d_model),
        )  # (B, T, Np, D)

        # 3) Concat: [latents, patch queries]  ->  (B, T, S=N_l+N_p, D)
        tokens = jnp.concatenate([latents, patches], axis=2)

        # 4) Add sinusoidal positions
        tokens = add_sinusoidal_positions(tokens)

        # 5) Axial block-causal transformer
        #    - SpaceSelfAttention over all S tokens (latents + queries)
        #    - TimeSelfAttention only over the first N_l latent tokens
        x = self.transformer(tokens, deterministic=deterministic)
        # 6) Prediction head over the patch-query slice
        x_patches = x[:, :, N_l:, :]                         # (B, T, Np, D)
        pred_btnd = nn.sigmoid(self.patch_head(x_patches))  # (B,T,Np,D_patch)
        return pred_btnd

class ActionEncoder(nn.Module):
    d_model: int
    n_keyboard: int = 4  # up, down, left, right (categorical actions)

    @nn.compact
    def __call__(
        self,
        actions: Optional[jnp.ndarray],           # (B, T) int32 in [0, n_keyboard)
        batch_time_shape: Optional[Tuple[int,int]] = None,
        as_tokens: bool = True,
    ):
        # Base "action token" embedding (used always)
        base_emb = self.param(
            'base_action_emb', nn.initializers.normal(0.02), (self.d_model,)
        )

        if actions is None:
            # unlabeled videos: just broadcast base embedding
            assert batch_time_shape is not None
            B, T = batch_time_shape
            out = jnp.broadcast_to(base_emb, (B, T, self.d_model))
        else:
            # embed categorical actions
            emb_key = nn.Embed(self.n_keyboard, self.d_model, name="emb_key")(actions)
            out = emb_key + base_emb  # broadcast add

        if as_tokens:
            # expand a token axis (S_a = 1)
            out = out[:, :, None, :]

        return out

class Dynamics(nn.Module):
    d_model: int              # dimensionality of each token
    d_bottleneck: int         # dimensionality of the input bottleneck space
    d_spatial: int            # dimensionality of each spatial token input
    n_s: int                  # number of spatial tokens
    n_r: int                  # number of learned register tokens
    n_heads: int
    depth: int
    k_max: int                 # maximum number of sampling steps (defines finest step 1/)
    dropout: float = 0.0
    mlp_ratio: float = 4.0
    time_every: int = 4
    latents_only_time: bool = False

    def setup(self):
        # Want to transform bottleneck inputs (B, T, N_b, D_b) to (B, T, N_b/packing_factor, D_b*packing_factor)
        assert self.d_spatial % self.d_bottleneck == 0
        self.packing_factor = self.d_spatial // self.d_bottleneck

        self.spatial_proj = nn.Dense(self.d_model, name="proj_spatial") # converts spatial tokens, of dim d_spatial to d_model
        self.register_tokens = self.param(
            "register_tokens",
            nn.initializers.normal(0.02),
            (self.n_r, self.d_model),
        )
        self.action_encoder = ActionEncoder(d_model=self.d_model)

        # Two separate tokens for shortcut conditioning (your current layout):
        self.layout = TokenLayout(n_latents=0, segments=(
                (Modality.ACTION, 1),
                (Modality.SHORTCUT_SIGNAL, 1),   # τ (signal level) token
                (Modality.SHORTCUT_STEP, 1),     # d (step size) token
                (Modality.SPATIAL, self.n_s),
                (Modality.REGISTER, self.n_r),
            )
        )
        self.spatial_slice = self.layout.slices()[Modality.SPATIAL]
        self.modality_ids = self.layout.modality_ids()

        self.transformer = BlockCausalTransformer(
            d_model=self.d_model,
            n_heads=self.n_heads,
            depth=self.depth,
            n_latents=0,
            modality_ids=self.modality_ids,
            space_mode="dynamics",
            dropout=self.dropout,
            mlp_ratio=self.mlp_ratio,
            latents_only_time=False,
        )

        # -------- Discrete embeddings for shortcut conditioning --------
        # Step size d ∈ {1, 1/2, 1/4, ..., 1/}
        # We index steps by: step_idx = log2(1/d) ∈ {0, 1, 2, ..., log2()}
        self.num_step_bins = int(jnp.log2(self.k_max)) + 1
        self.step_embed = nn.Embed(self.num_step_bins, self.d_model, name="step_embed")

        # Signal level τ ∈ {0, 1/d, 2/d, ..., 1 - 1/d} (grid length = 1/d)
        # We use a *shared* table with  bins and only use the first (1/d) entries for a given d.
        # Indexing: signal_idx = τ / d ∈ {0, 1, ..., (1/d) - 1}
        self.signal_embed = nn.Embed(self.k_max, self.d_model, name="signal_embed")
        self.flow_x_head = nn.Dense(self.d_spatial, name="flow_x_head")

    @nn.compact
    def __call__(self, enc_z, actions, signal_idx, step_idx, *, deterministic: bool = True):
        """
        Args:
          enc_z:      (B, T, N_l, D_b) encoder outputs before packing (e.g., 512×16)
          actions:    (B, T, N_a, D_a) raw action tokens
          signal_idx: (B, T) int32 — τ index on the grid for the chosen step size:
                        signal_idx = τ / d, ∈ {0, 1, ..., 1/d - 1}
          step_idx:   (B, T) int32 — step size index:
                        step_idx = log2(1/d), where d ∈ {1, 1/2, ..., 1/}

        Shapes produced:
          spatial_tokens: (B, T, n_s, d_model)
          action_tokens:  (B, T, 1, d_model)  # if your ActionEncoder emits one token
          signal_token:   (B, T, 1, d_model)
          step_token:     (B, T, 1, d_model)
        """
        # --- 1) Pack encoder tokens: (B, T, 512, 16) -> (B, T, 256, 32) as example
        packed_enc_tokens = rearrange(
            enc_z, 'b t (n_s k) d -> b t n_s (k d)', k=self.packing_factor, n_s=self.n_s
        )
        spatial_tokens = self.spatial_proj(packed_enc_tokens) # (B, T, n_s, d_model)

        # --- 2) Encode actions to d_model
        action_tokens = self.action_encoder(actions)  # (B, T, N_a, d_model)

        # --- 3) Prepare learned register tokens
        B, T = spatial_tokens.shape[:2]
        register_tokens = jnp.broadcast_to(
            self.register_tokens[None, None, ...],  # (1,1,n_r,d_model)
            (B, T, self.n_r, self.d_model),
        )

        # --- 4) Shortcut embeddings (discrete lookup)
        # Embed and add a singleton token dimension:
        step_tok   = self.step_embed(step_idx)[:, :, None, :]      # (B, T, 1, d_model)
        signal_tok = self.signal_embed(signal_idx)[:, :, None, :]     # (B, T, 1, d_model)

        # --- 5) Concatenate in your declared layout order
        tokens = jnp.concatenate(
            [action_tokens, signal_tok, step_tok, spatial_tokens, register_tokens],
            axis=2
        )

        tokens = add_sinusoidal_positions(tokens)      # (B, T, N_total, d_model)
        x = self.transformer(tokens, deterministic=deterministic)
        spatial_tokens = x[:, :, self.spatial_slice, :]
        readout = self.flow_x_head(spatial_tokens)
        return readout

def test_encoder_decoder():
    rng = jax.random.PRNGKey(0)
    B = 2
    T = 10
    n_patches = 4
    d_patch = 3
    enc_n_latents = 2
    enc_d_bottleneck = 3
    x = jnp.ones((B, T, n_patches, d_patch))  # (B,T,Np,D_patch)

    encoder = Encoder(d_model=8, n_latents=enc_n_latents, n_patches=n_patches, n_heads=2, depth=2, dropout=0.5, d_bottleneck=enc_d_bottleneck)
    decoder = Decoder(d_model=8, n_heads=2, depth=2, n_patches=n_patches, n_latents=enc_n_latents, d_patch=d_patch, dropout=0.5)
    # init: give both "mae" and "dropout" keys (dropout only needed if deterministic=False)
    enc_vars = encoder.init(
        {"params": rng, "mae": jax.random.PRNGKey(1), "dropout": jax.random.PRNGKey(2)},
        x,
        deterministic=True,
    )
    # Decode
    fake_z = jnp.ones((B, T, enc_n_latents, enc_d_bottleneck))
    dec_vars = decoder.init(
        {"params": rng, "dropout": jax.random.PRNGKey(2)},
        fake_z,
        deterministic=True,
    )

    def forward_apply(enc_vars: FrozenDict, dec_vars: FrozenDict,
                    patches_btnd: jnp.ndarray,
                    *, mae_key=None, drop_key=None, train: bool):
        # Encoder
        rngs_enc = {}
        if train:
            rngs_enc = {"mae": mae_key, "dropout": drop_key}
        else:
            rngs_enc = {"mae": mae_key}  # if you still want masking during eval

        z_btLd, mae_info = encoder.apply(enc_vars, patches_btnd,
                                        rngs=rngs_enc,
                                        deterministic=not train)
        # Decoder
        rngs_dec = {"dropout": drop_key} if train else {}
        pred_btnd = decoder.apply(dec_vars, z_btLd,
                                rngs=rngs_dec,
                                deterministic=not train)
        return pred_btnd, mae_info
    
    jit_forward = jax.jit(forward_apply, static_argnames=["train"])
    mae_key = jax.random.PRNGKey(0)
    drop_key = jax.random.PRNGKey(1)
    # Warm-up (compilation happens here)
    t0 = time.time()
    out = jit_forward(enc_vars, dec_vars, x, mae_key=mae_key, drop_key=drop_key, train=True)
    jax.tree_util.tree_map(lambda y: y.block_until_ready(), out)
    t1 = time.time()
    # Hot run (should be much faster)
    t2 = time.time()
    out = jit_forward(enc_vars, dec_vars, x, mae_key=mae_key, drop_key=drop_key, train=True)
    jax.tree_util.tree_map(lambda y: y.block_until_ready(), out)
    t3 = time.time()

    print(f"Warm-up (compile+run): {t1 - t0:.3f}s")
    print(f"Hot run (cached):      {t3 - t2:.3f}s")

    # print(pred.shape) # should be (B,T,Np,D_patch)
    # masked_pred = jnp.where(patch_mask, pred, 0.0)
    # masked_target = jnp.where(patch_mask, patch_tokens_btnd, 0.0)
    # num_masked = jnp.maximum(patch_mask.sum(), 1)
    # recon_loss = jnp.sum((masked_pred - masked_target) ** 2) / num_masked
    # print(recon_loss)
    # import ipdb; ipdb.set_trace()

def test_dynamics():
    rng = jax.random.PRNGKey(0)
    B = 2
    T = 10
    fake_enc_z = jnp.ones((B, T, 512, 16), dtype=jnp.float32)
    fake_actions = jnp.ones((B, T), dtype=jnp.int32)
    fake_signal_idx = jnp.ones((B, T), dtype=jnp.int32)
    fake_step_idx = jnp.ones((B, T), dtype=jnp.int32)
    # need some way to assert that 512 * 16 == 256 * 32
    dynamics_kwargs = {
        "d_model": 128,
        "n_s": 256,
        "d_spatial": 32,
        "d_bottleneck": 16,
        "k_max": 8,
        "n_r": 10,
        "n_heads": 4,
        "depth": 4,
        "dropout": 0.0
    }
    dynamics = Dynamics(**dynamics_kwargs)
    dynamics_vars = dynamics.init(
        {"params": rng, "dropout": jax.random.PRNGKey(2)},
        fake_enc_z,
        fake_actions,
        fake_signal_idx,
        fake_step_idx,
    )
    out = dynamics.apply(dynamics_vars, fake_enc_z, fake_actions, fake_signal_idx, fake_step_idx,
                        rngs={"dropout": jax.random.PRNGKey(2)},
                        deterministic=True)
if __name__ == "__main__":  
    # test_encoder_decoder()
    test_dynamics()